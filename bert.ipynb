{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5448551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635aeebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24c32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1772580",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = [line for line in dataset['text'] if len(line) > 50] \n",
    "subset_text = raw_text[:100000] # subset of 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b83e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s.lower().strip() for s in subset_text]\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7854fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(word_list):\n",
    "    word2id[w] = i + 4\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48daedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for sentence in sentences:\n",
    "    arr = [word2id[word] for word in sentence.split() if word in word2id]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4de787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "max_len = 128    # Maximum sequence length\n",
    "batch_size = 32\n",
    "max_mask = 5     # Max tokens to mask per sentence\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "d_model = 768\n",
    "d_ff = 768 * 4\n",
    "d_k = d_v = 64\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43794101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader Logic\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "\n",
    "    while len(batch) < batch_size:\n",
    "        idx_a = randrange(len(token_list))\n",
    "\n",
    "        # --- Correct NSP pairing ---\n",
    "        if random() < 0.5 and idx_a < len(token_list) - 1:\n",
    "            idx_b = idx_a + 1      # True next sentence (positive)\n",
    "            is_next = 1\n",
    "        else:\n",
    "            idx_b = randrange(len(token_list))  # Random sentence (negative)\n",
    "            is_next = 0\n",
    "\n",
    "        tokens_a, tokens_b = token_list[idx_a], token_list[idx_b]\n",
    "\n",
    "        tokens_a = tokens_a[:max_len//2 - 2]\n",
    "        tokens_b = tokens_b[:max_len//2 - 2]\n",
    "\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        cand_masked_pos = [i for i, token in enumerate(input_ids) if token not in range(4)]\n",
    "        shuffle(cand_masked_pos)\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "\n",
    "            if random() < 0.8:\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            elif random() < 0.5:\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index\n",
    "\n",
    "        if len(input_ids) > max_len:\n",
    "            input_ids = input_ids[:max_len]\n",
    "            segment_ids = segment_ids[:max_len]\n",
    "\n",
    "            valid = [i for i, p in enumerate(masked_pos) if p < max_len]\n",
    "            masked_pos = [masked_pos[i] for i in valid]\n",
    "            masked_tokens = [masked_tokens[i] for i in valid]\n",
    "\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        if max_mask > len(masked_tokens):\n",
    "            n_pad = max_mask - len(masked_tokens)\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if is_next == 1 and positive < batch_size // 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, 1])\n",
    "            positive += 1\n",
    "        elif is_next == 0 and negative < batch_size // 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, 0])\n",
    "            negative += 1\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4070ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT Architecture\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=device).unsqueeze(0).expand_as(x)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    return seq_k.data.eq(0).unsqueeze(1).expand(batch_size, len_q, len_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7756ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        return torch.matmul(attn, V), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5318462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        context, _ = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4437665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd864603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75dda309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # Decoder weights tied to embeddings\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, enc_self_attn_mask)\n",
    "        \n",
    "        h_pooled = self.activ(self.fc(output[:, 0]))\n",
    "        logits_nsp = self.classifier(h_pooled)\n",
    "        \n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        h_masked = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "        \n",
    "        return logits_lm, logits_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018f1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "model = BERT().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c05ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 68.2752\n",
      "Epoch 2, Loss: 23.4773\n",
      "Epoch 3, Loss: 16.0118\n",
      "Epoch 4, Loss: 13.3716\n",
      "Epoch 5, Loss: 12.0592\n",
      "Epoch 6, Loss: 11.1732\n",
      "Epoch 7, Loss: 10.7772\n",
      "Epoch 8, Loss: 10.4050\n",
      "Epoch 9, Loss: 10.0351\n",
      "Epoch 10, Loss: 9.8702\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10):  \n",
    "    total_loss = 0\n",
    "\n",
    "    for _ in range(100):\n",
    "        batch = make_batch()\n",
    "\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "            map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        masked_tokens = masked_tokens.to(device)\n",
    "        masked_pos = masked_pos.to(device)\n",
    "        isNext = isNext.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "        loss_lm = criterion(\n",
    "            logits_lm.view(-1, vocab_size),\n",
    "            masked_tokens.view(-1)\n",
    "        )\n",
    "        loss_nsp = criterion(logits_nsp, isNext)\n",
    "\n",
    "        loss = loss_lm + loss_nsp\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f6a57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Weights Saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'bert_from_scratch.pth')\n",
    "print(\"BERT Weights Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f43a3",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e11950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SentenceBERT(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_size, num_classes=3):\n",
    "        super(SentenceBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(hidden_size * 3, num_classes)\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def forward(self, input_ids_a, seg_ids_a, input_ids_b, seg_ids_b):\n",
    "        # Siamese structure: SAME BERT for both sentences\n",
    "        u_output = self.get_bert_embeddings(input_ids_a, seg_ids_a)\n",
    "        v_output = self.get_bert_embeddings(input_ids_b, seg_ids_b)\n",
    "\n",
    "        u = self.mean_pooling(u_output, (input_ids_a != 0))\n",
    "        v = self.mean_pooling(v_output, (input_ids_b != 0))\n",
    "\n",
    "        uv_abs = torch.abs(u - v)\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1)\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def get_bert_embeddings(self, input_ids, segment_ids):\n",
    "        x = self.bert.embedding(input_ids, segment_ids)\n",
    "        attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.bert.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5365f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli = load_dataset(\"snli\", split=\"train\")\n",
    "snli = snli.filter(lambda x: x['label'] != -1).select(range(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662bfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Tokenizer\n",
    "import re\n",
    "\n",
    "def tokenize_snli(example):\n",
    "    def encode(text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "        tokens = text.split()\n",
    "        return [word2id.get(w, word2id['[MASK]']) for w in tokens][:max_len]\n",
    "\n",
    "    def pad(ids):\n",
    "        return ids + [0] * (max_len - len(ids))\n",
    "\n",
    "    ids_a = encode(example['premise'])\n",
    "    ids_b = encode(example['hypothesis'])\n",
    "\n",
    "    return {\n",
    "        'input_ids_a': pad(ids_a),\n",
    "        'input_ids_b': pad(ids_b),\n",
    "        'label': example['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "496b6d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20000/20000 [00:00<00:00, 30478.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_snli = snli.map(tokenize_snli)\n",
    "tokenized_snli.set_format(type='torch', columns=['input_ids_a', 'input_ids_b', 'label'])\n",
    "loader = DataLoader(tokenized_snli, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f007c962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading BERT weights\n",
    "base_bert = BERT().to(device)\n",
    "base_bert.load_state_dict(torch.load('bert_from_scratch.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10a106ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceBERT(base_bert, d_model).to(device)\n",
    "optimizer = optim.Adam(sbert_model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ace0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Epoch 1, Loss: 1.2335\n",
      "SBERT Epoch 2, Loss: 1.1053\n",
      "SBERT Epoch 3, Loss: 1.1028\n",
      "SBERT Epoch 4, Loss: 1.0648\n",
      "SBERT Epoch 5, Loss: 1.0275\n",
      "SBERT Epoch 6, Loss: 1.0029\n",
      "SBERT Epoch 7, Loss: 0.9810\n",
      "SBERT Epoch 8, Loss: 0.9591\n",
      "SBERT Epoch 9, Loss: 0.9412\n",
      "SBERT Epoch 10, Loss: 0.9148\n"
     ]
    }
   ],
   "source": [
    "#Training Loop (SoftmaxLoss objective)\n",
    "sbert_model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        ids_a = batch['input_ids_a'].to(device)\n",
    "        ids_b = batch['input_ids_b'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        seg_a = torch.zeros_like(ids_a).to(device)\n",
    "        seg_b = torch.zeros_like(ids_b).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = sbert_model(ids_a, seg_a, ids_b, seg_b)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"SBERT Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05c3b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-BERT Saved\n"
     ]
    }
   ],
   "source": [
    "#Save Sentence-BERT model\n",
    "torch.save(sbert_model.state_dict(), 'sbert_model.pth')\n",
    "print(\"Sentence-BERT Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9899f",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6791f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Vocab size: 66143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 15761.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "PERFORMANCE METRICS\n",
      "========================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.48      0.65      0.55       344\n",
      "      neutral       0.42      0.49      0.45       327\n",
      "contradiction       0.48      0.22      0.31       329\n",
      "\n",
      "     accuracy                           0.46      1000\n",
      "    macro avg       0.46      0.46      0.44      1000\n",
      " weighted avg       0.46      0.46      0.44      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Vocab size: {len(word2id)}\")\n",
    "except NameError:\n",
    "    print(\"error\")\n",
    "\n",
    "def tokenize_snli_eval(example):\n",
    "    tokens_a = example['premise'].lower().split()\n",
    "    tokens_b = example['hypothesis'].lower().split()\n",
    "    \n",
    "    ids_a = [word2id.get(w, word2id['[MASK]']) for w in tokens_a][:max_len]\n",
    "    ids_b = [word2id.get(w, word2id['[MASK]']) for w in tokens_b][:max_len]\n",
    "    \n",
    "    # Padding\n",
    "    ids_a += [0] * (max_len - len(ids_a))\n",
    "    ids_b += [0] * (max_len - len(ids_b))\n",
    "    \n",
    "    return {\n",
    "        'input_ids_a': torch.tensor(ids_a),\n",
    "        'input_ids_b': torch.tensor(ids_b),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "test_dataset = load_dataset(\"snli\", split=\"test\")\n",
    "test_dataset = test_dataset.filter(lambda x: x['label'] != -1).select(range(1000)) # Using 1000 for speed\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_snli_eval)\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids_a', 'input_ids_b', 'label'])\n",
    "test_loader = DataLoader(tokenized_test, batch_size=32)\n",
    "\n",
    "#evaluation\n",
    "sbert_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids_a = batch['input_ids_a'].to(device)\n",
    "        ids_b = batch['input_ids_b'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        seg_a = torch.zeros_like(ids_a).to(device)\n",
    "        seg_b = torch.zeros_like(ids_b).to(device)\n",
    "        \n",
    "        logits = sbert_model(ids_a, seg_a, ids_b, seg_b)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "target_names = ['entailment', 'neutral', 'contradiction']\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
